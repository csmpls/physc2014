
1. resist writing in a conversational tone; provide specifics where possible, e.g., accuracy declined by 2.5% when data compressed by 1000%; increasing training time from 3 minutes to 30 minutes only improved accuracy from 90% to 92%.

2. practice to write "defensively". This is analogous to driving defensively. Be aware of the claims that you are making, and make sure that they are precise and defensible. It is better to err on making a narrower claim than one that may be challenged by the reviewers. Also, you should always assume that the authors of the referenced papers will also be the reviewers of the paper, because they very often are. So, when you describe the shortcomings of a cited work, you should feel confident of making that same criticism while you are standing up in a full room, with the authors of the cited work in the room. Otherwise, see if you can reword the sentence to get your point across.

Below are some specific comments.

Section 1.

You can come back to iterate on this introductory section (context, research questions, contributions, which may include a summary of key techniques and main findings) after you have taken a pass through the rest of the paper

.
.
.


.


Section 3.

.

. . . . . .; . . . 

.

.


Section 3.3:

The first paragraph that provides background on SVM may be unnecessary for the target audience. Instead, I think you should provide an overview to the three experiments, what are the research questions or hypotheses you are investigating with these classification experiments, and why you chose these three experiments in particular.

You may also want to rename the section title from "classifier" to something more descriptive of the experiments.

For the next three sections (3.4-3.6), it will be helpful to start each section by expanding on what the particular experiment is about, before going into the step-by-step descriptions. For example, in the first experiment, you may want to say that we want to understand how the level of data compression, operationally controlled by the number of bins, affect the accuracy and calibration time of classification. In the second experiment, you want to clarify how it is different from the first, and what different research question it is designed to answer. Same thing with the third experiment. It may be helpful to give names to each experiment, as opposed to just numbering them 1-3. 

As you go through these sections, you should think about whether we should call them "experiments" or something else, e.g., "strategies", instead.


Section 4:

First, some general comments. I wonder if the text will flow better if you integrated the methods and results for each experiment. It may also make it easier for you to write and justify the latter experiments after you have described the methods and results of the earlier experiments.

Figures: I cannot read the axis labels and scales. Please enlarge them for this middle-aged guy please. Thanks! :-)


Experiment 1 results:

Please put back the graphs for "accuracy vs bin-size" and "training time vs bin-size", so that it is clear that both of these performance measures are dependent on bin size. Then, you can turn to the third graph of accuracy vs training time, and make the point that there is a tradeoff here. If you want, you can number them as Figures 1(a), 1(b), and 1(c).

Also, in addition to presenting the numerical results, you can also add a couple of sentences on the takeaways of these results, before moving on to the next section.

Experiment 2 results:

The paragraph does not appear to match the graph in Figure 2.

What I infer from Figure 2 is this:

- if we use the full 60 seconds for training, we can get 94-96% accuracy, across different bin sizes;
- if we reduce it down to 10 seconds, we can get 83-86% accruacy;
- so once again, there is a tradeoff between accuracy and training time

The figure also raises a question for me: why is it that most of the accuracy loss occurs from 60s to 50s, particularly for larger bin-sizes? 

Question: How can we compare the results from experiment 2 from those from experiment 1? Or are they not comparable?

What is the takeaway of this experiment? 


Experiment 3 results:

Perhaps this strategy should be named the "Greedy Strategy" or "Opportunistic Strategy". It seems to me that the graphs in Figure 3 do not match the objective of this strategy. Aren't we trying to show that we can minimize training time and still get good accuracy with this approach, and that it works very well for some significant subset of the subject population? 

Section 5.

Paragraph 1: back up the qualitative claims with quantitative numbers from the preceding sections.

Paragraph 2: "Finally, we did not compare our findings to traditional signal processing methods in EEG." --> This is a suicidal statement to make. The reviewers will pounce on this. You need to rephrase it to be more specific as to the scope of this statement, and provide justification.

Hope this helps!
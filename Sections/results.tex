\section{\uppercase{Effect of quantization on classifier speed and accuracy}}
%\section{\uppercase{The effect of quantization technique on classifier speed and accuracy}}
\label{sec:quantization_eval}

%Generally, we seek to maximize our system's classification accuracy while minimizing its computational expense. One way to reduce the computational requirements of a SVM classifier is to reduce the size of the feature vectors on which it is trained and tested. Our signal quantization method allows us to directly adjust the size of feature vectors by changing the signal's resolution (see 3.1), though lowering the resolution of feature vectors could negatively effect the classifier's performance.

We examine the effect of signal resolution, operationalized by the number of bins used in the quantization step, on our BCI's performance, measured by the SVM's training time and by the SVM's estimated accuracy. 
We hypothesize that both the SVM training time and accuracy increase with signal resolution, i.e., the greater the number of bins, the higher the accuracy but also the longer the training time.

%H1: SVM accuracy will decrease with resolution.
%H2: SVM training time will decrease with resolution.

%\subsection{Protocol}

For each subject, we generate every pair of two mental tasks and cross-validate our SVM on the recordings for this pair of tasks. Given the availability of seven candidate tasks, we have a total of 21 possible task pairs. For every task pair, we vary the signal resolution by varying the number of bins from 1 to 1024. For every task pair processed, we record mean classification accuracy across all rounds of cross-validation. For each subject, we record the best-performing task pair, which corresponds to our estimation of optimal performance of the BCI for that subject.

%For each subject, we generate every pair of two tasks and cross-validate our SVM seven times on the recordings for this pair of tasks. We vary the resolution of the samples we feed to the SVM. For every task pair processed, we record mean classification accuracy across all rounds of cross-validation. For each subject, we record the best-performing taskpair, which corresponds to our estimation of optimal performance of the BCI for that subject.

As an additional performance audit, we measure the time needed to fit an SVM to the data for two randomly selected task pairs across all subjects. We repeat this process ten thousand times at different resolutions, collecting the minimum time observed in each series of attempts.

%\subsection{Results}

Figure \ref{fig:accuracy_vs_bins} shows the mean best-case accuracy of the classifier versus the number of bins. We can see that the accuracy level remains above 90\% even as we reduce the signal resolution down to 100 bins. Although classifier accuracy is positively correlated with signal resolution (Slope = 0.0013, R-squared = 0.773, p \textless 0.001), this effect appears only at resolutions lower than 100 bins. We find no significant difference in SVM accuracy at resolutions over 100 bins.

%Figure \ref{fig:accuracy_vs_bins} shows the mean best-case accuracy of the classifier versus the number of bins. Although classifier accuracy is positively correlated with signal resolution (Slope = 0.0013, R-squared = 0.773, p \textless 0.001), this effect appears only at resolutions lower than 100 bins. We find no significant difference in SVM accuracy at resolutions over 100 bins.

%We find support for H1. Although resolution was positively correlated with classifier accuracy (slope = .0013 R-squared = .773, p \textless .001), this effect appears only at resolutions lower than 100 points. We find no significant increase in SVM accuracy at resolutions over 100 bins. 

Figure \ref{fig:training_vs_bins} shows, in log-log scale, the SVM training time versus the number of bins. We see that the log of the classifier training time is positively correlated with the log of signal resolution (Slope = 0.5, R-squared = 0.947, p \textless 0.001). 

Combining these two results, Figure \ref{fig:accuracy_vs_training} confirms the direct tradeoff between classifier accuracy and classifier training time. It also points to the existence of a threshold resolution at around 100 bins that provides a 450\% speed improvement over a non-quantized baseline of 1024 bins, without any significant degradation in classifier accuracy.

%Resolution was also positively correlated with time to train classifier (slope = 0.5 R-squared = 0.947, p \textless .001). We compare accuracy and SVM training time directly in \ref{fig:fig1c}. Thus, we find support for H2.

\begin{figure}[!h]
%  \vspace{-0.2cm}
  \centering
   {\epsfig{file = Figures/1a.png, width = 6cm}}
  \caption{Mean best-case accuracy among all subjects compared to time needed to train the classifier. At signal resolutions of 100 points (bins) and greater, we find no evidence of an increase in classification accuracy. }
  \label{fig:accuracy_vs_bins}
  \vspace{-0.1cm}
 \end{figure}

 \begin{figure}[!h]
  \vspace{-0.2cm}
  \centering
   {\epsfig{file = Figures/1b.png, width = 6cm}}
  \caption{Log of mean classifier training time compared to log of data resolution. The slope is 0.5, implying that the time needed to train the classifier increases as approximately the square root of the signal resolution.}
  \label{fig:training_vs_bins}
  \vspace{-0.1cm}
 \end{figure}

\begin{figure}[!h]
  \vspace{-0.2cm}
  \centering
   {\epsfig{file = Figures/1c.png, width = 6cm}}
  \caption{ Best-case accuracy compared to the time needed to train the classifier. By decreasing the number of bins in the EEG data, we can decrease the time needed to train the support vector machine up to nine times without without significant detriment to classifier accuracy. }
  \label{fig:accuracy_vs_training}
%  \vspace{-0.1cm}
 \end{figure}

Overall, we find that relatively small feature vectors produced with our method (100 values) yield classifiers as accurate as full-resolution samples (1024 values), and that reducing vector size in this way can dramatically increase the computational speed of training an SVM. 


\section{\uppercase{Opportunistic strategy for calibrating a binary BCI}}
\label{sec:calibration_eval}

\noindent In the previous section, we find that our compression technique can speed up an SVM classifier without significant detriment to BCI accuracy. However, for the technique to be useful in real-world applications, it must also allow users to quickly calibrate the system to their personal physiological signals.

In this section, we evaluate an opportunistic strategy for user calibration. Using the logarithmic binning method to produce quantized signals with a resolution of 100 bins, we measure user calibration time (the time it takes a user to achieve a threshold accuracy with the BCI) and the classification accuracy each user achieves after calibration. We hypothesize that an opportunistic approach will allow for faster calibration than an exhaustive search while maintaining sufficient system accuracy across users.

%In this section, we evaluate an opportunistic strategy for user calibration. Using a resolution of 100 points identified as optimal in the previous experiment, we measure user calibration time (the time it takes a user to achieve a threshold accuracy with the BCI) and the classification accuracy each user achieves after calibration. We hypothesize that this technique will allow for faster calibration than an exhaustive search (720 seconds) while maintaining sufficient system accuracy across users.

%\subsection{Protocol}

As a baseline, we perform an exhaustive search of SVM accuracy on mental task pairs, and identify each subject's best-performing task pair. We record the frequency of each task's occurrence in a best-case task pair in Table \ref{table:name}. Assuming that we can establish a consistent ordering of best performing tasks for a target population, we use this data to inform the order in which our opportunistic calibration strategy would prompt the user to perform the mental tasks.

% this is where i put a table of tasks correlated with bestcase performance and a quick description, how we generated those data and what we need them for

\begin{table}[!h]
%  \vspace{-0.2cm}
  \centering
  \begin{tabular}{ | l | l | l | p{5cm} |}
  \hline
  Task & Frequency \\ \hline
  \textit{color} & 10 \\ \hline
  \textit{breathing} & 5 \\ \hline
  \textit{pass} & 4 \\ \hline
  \textit{sport} & 3 \\ \hline
  \textit{finger} & 2 \\ \hline
  \textit{song} & 2 \\ \hline
  \textit{audio} & 2 \\ \hline
  \end{tabular}
  \caption{Frequency where a mental task occurs in a task pair that achieves a highest classification accuracy, across all possible task pairs.}
%  \caption{An exhaustive search of SVM accuracy on taskpairs identified each subject's best-performing taskpair. We recorded the frequency of each task's occurrence in a best-case taskpair, shown here. We used these dataÂ to inform the order in which our opportunistic calibration strategy would prompt the user to record tasks.}
  \label{table:name}
%  \vspace{-0.1cm}
\end{table}

The opportunistic strategy starts with three tasks most commonly associated with best-case performance (\textit{color}, \textit{breathing}, \textit{pass}) for an initial user calibration time of 120 seconds (40 seconds per task). We then perform a seven-fold cross-validation on every permutation of two of these tasks (i.e., \textit{color} versus \textit{breathing}, \textit{color} versus \textit{pass}, \textit{breathing} versus \textit{pass}). The task pair with the highest mean score across cross-validation rounds is selected for an additional testing session, in which the remaining 80 seconds of recordings for both tasks are used to generate an estimate of the classifier's accuracy on new EEG signals.

If the score on this additional testing round is below 75\%, a commonly used threshold for BCI literacy \cite{vidaurre_towards_2010}, the user will be prompted to record sixty seconds of the next candidate mental task. We repeat the above process on unexplored tasks until a task pair achieves over 75\% accuracy on post-calibration data, or until all tasks have been evaluated.

We can test two hypotheses regarding this opportunistic calibration strategy.

H1: The opportunistic calibration strategy will reach threshold accuracy in less time than will the exhaustive search method.

H2: The opportunistic calibration strategy will achieve lower accuracy than will the exhaustive search method, as it could find local optima.

%\subsection{Results}

For the given set of seven candidate mental tasks, the baseline exhaustive search strategy requires 2520 seconds of calibration time and produces an average accuracy of 92.5\% across subjects (\textit{std} = 0.09). Our opportunistic strategy takes an average of 225.3 seconds of calibration time (\textit{std} = 52.2) and produces an average accuracy of 88.3\% (\textit{std} = 0.11).

Figure \ref{fig:calibration_results} shows the results from a subject's perspective. Out of 15 subjects, the opportunistic calibration strategy allows 73.6\% (11 subjects) to be calibrated in under 4 minutes, and 86.6\% (13 subjects) in under 5 minutes. The remaining two subjects were calibrated under 10 and 15 minutes, respectively. All 15 subjects can achieve a minimum of 75\% classification accuracy. Six subjects (40\%) can achieve 100\% accuracy.

\begin{figure}[!h]
  \vspace{-0.2cm}
  \centering
   {\epsfig{file = Figures/3.png, width = 5.5cm}}
  \caption{Calibration time across subjects (top) and classifier accuracy (bottom). The vast majority of subjects achieve acceptable accuracy in under five minutes of training, and all subjects achieve BCI literacy in under 15 minutes. }
  \label{fig:calibration_results}
  \vspace{-0.1cm}
\end{figure}

We find that our strategy calibrates users to BCI control significantly more quickly than an exhaustive search, finding support for H1 at the \textit{p} \textless 0.001 level. On the other hand, we do not find a statistically significant difference in per-user accuracy between an opportunistic strategy and an exhaustive search (\textit{p-value} = 0.264). Thus, we find no support for H2.



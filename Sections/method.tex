\section{\uppercase{Method}}

This section details the signal extraction technique that this study evaluates and describes the machine learning classifier used in our binary BCI.

\subsection{Signal extraction}

\subsubsection{Compressing power spectra in the temporal dimension}

For each bin of 1/4 Hz, we compute a median value out of the n power spectra generated for one sample. We obtain a discrete probability density function (PDF) with each bin being the median of the corresponding bins in the n power spectra. At this stage, we have a discrete PDF of 1024 bins for the whole sample. This method represents a ``stacking" of several PDFs into one PDF that represents the statistical average of all the others.

\subsubsection{Logarithmic binning}

Binning the PDF is a simple way to ``quantize'' the information contained in the original power spectrum. By taking the median of several bins, we are left with a single bin that compresses the information of its neighbors. For instance, four contiguous bins (1-1.25,1.25-1.5,1.5-1.75,1.75-2) have the values (4,4,5,5) the value resulting from combining these values into one bin would be 4.5. 

It is desirable to arrange the bins such that they provide relevant information on the whole distribution. Because the PDF is heavy-tailed, one way to do this efficiently is to arrange the compression bins in a logarithmic fashion. 

% thomas notes: I think here is the originality of the method: we take all the power spectrum with one advantage and one disadvantage. The advantage is that we have a much larger spectrum to characterize and classify tasks, at the cost of probably more pollution from non-EEG signal from the environment, from muscles, etc.]}

\begin{figure}[!h]
  \vspace{-0.2cm}
  {\epsfig{file = Figures/binned_EEGPowerSpectrum.png, width = 6cm}}
\caption{In double logarithmic scale, the original 1024 bins (blue dots) of the PDF obtained from averaging the n power spectra of one recording, and the resulting ``quanitized''  PDF with a resolution of 100 log-bins. The quantized PDF preserves very well the structure of the original, 1024-point PDF. }
\label{binnedEEGpowerspec}
\vspace{-0.1cm}
\end{figure}

In summary, we build a probability density function of brain frequencies as captured by the Neurosky hardware, from the n power spectra of each sample. We then used a log-binning method to reduce the 1024 bins from the original power spectrum to an arbitrary smaller number of log-bins (e.g., 100 log-bins on the Figure). This method makes a sort of statistical averaging by stacking, and then compresses the result in way that it is easy to use in a classifier.


% {\bf NB:} I believe that the classifier does well because it can efficiently capture the overall level of activity for all log-bins, but also more local deviations. On the figure, we can see some local peaks mostly between $10^1$ and $10^1.5$, but in all other parts of the PDF though in a less visible way. I believe these deviations are quite unique and can make the difference in the classifier. We should indeed check this further if we want to understand to origins of the good results.

% {\bf Side note:} one way to investigate further would be to see indeed to what extent the ``compression", i.e. the small number of log-bins, affects the quality of the classifier. Another quite promising further research direction, would be to determine the minimum number of n power spectra, which should be taken into account to reach a target level of correct classification. This would be useful to determine what should be the most adequate sample size for a certain level of identification. This level might of course vary as a function of subjects and tasks.

%-----------


% \textcolor{red}{\bf [Maybe a schema would be great to help the reader get the point quickly]}


\subsection{Classifying EEG signals}


In this study, we build a binary BCI using a support vector machine (SVM) classifier, which we train individually for each subject. SVMs work by drawing a discriminatory boundary (a ``hyperplane'') between training examples such that the margin of the hyperplane is maximized for all examples in the set. (For more on the use of SVMs in BCI: \cite{garrett_comparison_2003,grierson_better_2011}). 

\subsubsection{LibLinear}
We use LinearSVC, \cite{fan_liblinear:_2008} a wrapper for LibLinear exposed in Python through the ScikitLearn library. \cite{pedregosa_scikit-learn:_2011} We chose LinearSVC because BCI classification problems are generally presumed to be linear  \cite{garrett_comparison_2003,lotte_review_2007}, and because LibLinear's underlying C implementation boasts among the fastest train- and test-time performance among state-of-the-art solutions. \cite{fan_liblinear:_2008} For the SVM's paramaters, we use a squared hinge loss function, which maximizes the margin for greater generalizability, and a hyperparameter of 100, found through a ``grid search'', or an exhaustive search through a randomly selected sample of our dataset. 

\subsubsection{Cross-validation}

\textcolor{red}{\bf explanation of why we bother with cross validation, and what it means to estimate the accuracy of a classifier }

We use ScikitLearn's built-in cross-validation toolkit, which performs each of the seven cross-validation steps using different splits of trial data in the training and testing sets. 

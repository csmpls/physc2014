\section{\uppercase{Method}}

\subsection{Signal extraction}

The Neurosky MindSet SDK delivered a power spectrum of its data every half second. Offline, we compress the data in the temporal dimension, taking the middle \textit{n} seconds of the recording, where $n = \{0.5, 1.0,1.5, 2.0,...,8.0, 8.5, 9.0\}$. 

The Neurosky software computes a power spectrum every half second. The maximum frequency is 256Hz, as the maximum sampling rate of Neurosky hardware is 512Hz. The power spectrum is computed with discrete bins of 1/4 Hz. Each bin represents the intensity of activation of a frequency range (e.g., between 1 and 1.25 Hz) in a half-second time window. There are therefore 1024 values reported for one power spectrum. Our samples are more or less 10 seconds, which means around 20 power spectra computed per sample.

Thereafter, the signal extraction method consists of two main steps: (i) to build a statistical significance out of the n power spectra produced in each sample, and (ii) to ``compress'' the information into a smaller vector size using a median filter. For each bin of 1/4 Hz, we can compute a median value out of the n power spectra generated for one sample. We obtain a discrete probability density function (pdf) with each bin being the median of the corresponding bins in the n power spectra. At this stage, we have a discrete pdf of 1024 bins for the whole sample. This method represents a ``stacking" of several probability density functions into one representing the statistical average of all the others.

Binning the pdf is a simple way to ``compress" the information contained in the original power spectrum. The basic idea is to take the median of several bins. For instance, four contiguous bins (1-1.25,1.25-1.5,1.5-1.75,1.75-2) have the values (4,4,5,5) the value resulting from combining these values into one bin would be 4.5. The pdf is heavy-tailed and it is desirable to arrange the ``compression" bins in a way it provides relevant information on the whole distribution. One way to do this efficiently is to arrange the compression bins in a logarithmic fashion.

% thomas notes: I think here is the originality of the method: we take all the power spectrum with one advantage and one disadvantage. The advantage is that we have a much larger spectrum to characterize and classify tasks, at the cost of probably more pollution from non-EEG signal from the environment, from muscles, etc.]}

\begin{figure}
\begin{center}
\includegraphics[width=5in]{Figures/binned_EEGPowerSpectrum.png}
\caption{Binned power spectrum}
\label{binnedEEGpowerspec}
\end{center}
\end{figure}

%thomas notes: About here, a short pitch on pink noise is required to set the stage of the great method, and to justify the log log binning]}

Figure \ref{binnedEEGpowerspec} shows, in double logarithmic scale, the original 1024 bins (blue dots) of the pdf obtained from averaging the n power spectra of one sample, and the resulting ``compressed''  pdf with 100 log-bins. As we can see, the log-binning preserves the structure of the pdf.

In summary, we build a probability density function of brain frequencies as captured by the Neurosky hardware, from the n power spectra of each sample. We then used a log-binning method to reduce the 1024 bins from the original power spectrum to an arbitrary smaller number of log-bins (e.g., 100 log-bins on the Figure). This method makes a sort of statistical averaging by stacking, and then ``compresses" the result in way that it is easy to use in a classifier.

% {\bf NB:} I believe that the classifier does well because it can efficiently capture the overall level of activity for all log-bins, but also more local deviations. On the figure, we can see some local peaks mostly between $10^1$ and $10^1.5$, but in all other parts of the pdf though in a less visible way. I believe these deviations are quite unique and can make the difference in the classifier. We should indeed check this further if we want to understand to origins of the good results.

% {\bf Side note:} one way to investigate further would be to see indeed to what extent the ``compression", i.e. the small number of log-bins, affects the quality of the classifier. Another quite promising further research direction, would be to determine the minimum number of n power spectra, which should be taken into account to reach a target level of correct classification. This would be useful to determine what should be the most adequate sample size for a certain level of identification. This level might of course vary as a function of subjects and tasks.

%-----------


% \textcolor{red}{\bf [Maybe a schema would be great to help the reader get the point quickly]}

At this point, we have a one-dimensional signal flattened in the time dimension, computing the mean magnitude of each bin over all readings in the recording - a row vector with one entry for each each measured frequncy bin. 

\subsection{Strategies for BCI with minimal training time and low computational requirements}

Our study explores three major questions. First, what is the tradeoff between data compression and classification accuracy? For computational reasons, we wish to keep the size of our data small, but what is the optimal compromise between this computational concern and the usability of the resulting BCI?

Second, how well do the data we record during calibration represent the data we will see after calibration? From an ergonomic standpoint, we wish to keep the duration of example recordings as brief as possible. With how little initial data can we confidently make inferences about data we see in the future, and does our compression method affect our ability to make these inferences?

Third, how can we minimize the time the user spends training while maximizing the accuracy of the resulting system? For ergonomic reasons, it is desireable that the user not spend too much time calibrating the system (recording example data, testing BCI accuracy). Can we design our calibration phase to keep training times low and classification accuracies high across all users?

We explore these research questions using a binary BCI built around a support vector machine trained per-subject on examples from our dataset. (For more on the use of SVMs in BCI: \cite{garrett_comparison_2003,grierson_better_2011}). In this study, we use LinearSVC, \cite{fan_liblinear:_2008} a wrapper for LibLinear exposed in Python through the ScikitLearn library. \cite{pedregosa_scikit-learn:_2011} We chose LinearSVC primarily because its underlying C implementation is very performant. We use the default settings for LinearSVC: a C of 1.0, squared hinge loss function, and a tolerance parameter of of 1e-4. 

% TODO: provide evidence for "as well or better than nonlinear ones"
% generally presumed to be linear  \cite{garrett_comparison_2003,lotte_review_2007}
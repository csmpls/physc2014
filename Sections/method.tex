\section{\uppercase{Method}}

The data used in this experiment were taken from a previous study. As our research involved human subjects, our experimental procedures were approved by an Institutional Review Board. We recruited 15 subjects to participate in our study, all of whom were UC Berkeley undergraduate or graduate students. Each subject met with investigators in a quiet, closed room for two 40-50 minute sessions on two separate days. We briefed subjects on the objective of the study, fitted them with a Neurosky MindSet headset, and provided instructions for completing each task. As the subjects performed each task, we recorded readings from the headset (i.e., difference of potential and power spectrum every half second).

\subsection{Tasks}

% TODO: steal from passthoguhts

\subsection{Signal extraction}

The Neurosky MindSet SDK delivered a power spectrum of its data every half second. Offline, we compress the data in the temporal dimension, taking the middle \textit{n} seconds of the recording, where $n = \{0.5, 1.0,1.5, 2.0,...,8.0, 8.5, 9.0\}$. 

The Neurosky software computes a power spectrum every half second. The maximum frequency is 256Hz, as the maximum sampling rate of Neurosky hardware is 512Hz. The power spectrum is computed with discrete bins of 1/4 Hz. Each bin represents the intensity of activation of a frequency range (e.g., between 1 and 1.25 Hz) in a half-second time window. There are therefore 1024 values reported for one power spectrum. Our samples are more or less 10 seconds, which means around 20 power spectra computed per sample. Based on the signal quality also recorded some power spectra are removed (Benjamin knows the exact filtering method).

Thereafter, the signal extraction method consists of two main steps: (i) to build a statistical significance out of the n power spectra produced in each sample, and (ii) to ``compress'' the information into a smaller vector size using a median filter. For each bin of 1/4 Hz, we can compute a median value out of the n power spectra generated for one sample. We obtain a discrete probability density function (pdf) with each bin being the median of the corresponding bins in the n power spectra. At this stage, we have a discrete pdf of 1024 bins for the whole sample. This method represents a ``stacking" of several probability density functions into one representing the statistical average of all the others.

Binning the pdf is a simple way to ``compress" the information contained in the original power spectrum. The basic idea is to take the median of several bins. For instance, four contiguous bins (1-1.25,1.25-1.5,1.5-1.75,1.75-2) have the values (4,4,5,5) the value resulting from combining these values into one bin would be 4.5. The pdf is heavy-tailed and it is desirable to arrange the ``compression" bins in a way it provides relevant information on the whole distribution. One way to do this efficiently is to arrange the compression bins in a logarithmic fashion.

% thomas notes: I think here is the originality of the method: we take all the power spectrum with one advantage and one disadvantage. The advantage is that we have a much larger spectrum to characterize and classify tasks, at the cost of probably more pollution from non-EEG signal from the environment, from muscles, etc.]}

% \begin{figure}
% \begin{center}
% \includegraphics[width=5in]{Figures/binned_EEGpowerspectrum.png}
% \caption{Binned power spectrum}
% \label{binnedEEGpowerspec}
% \end{center}
% \end{figure}

%thomas notes: About here, a short pitch on pink noise is required to set the stage of the great method, and to justify the log log binning]}

Figure \ref{binnedEEGpowerspec} shows, in double logarithmic scale, the original 1024 bins (blue dots) of the pdf obtained from averaging the n power spectra of one sample, and the resulting ``compressed''  pdf with 100 log-bins. As we can see, the log-binning preserves the structure of the pdf.

In summary, we build a probability density function of brain frequencies as captured by the Neurosky hardware, from the n power spectra of each sample. We then used a log-binning method to reduce the 1024 bins from the original power spectrum to an arbitrary smaller number of log-bins (e.g., 100 log-bins on the Figure). This method makes a sort of statistical averaging by stacking, and then ``compresses" the result in way that it is easy to use in a classifier.

% {\bf NB:} I believe that the classifier does well because it can efficiently capture the overall level of activity for all log-bins, but also more local deviations. On the figure, we can see some local peaks mostly between $10^1$ and $10^1.5$, but in all other parts of the pdf though in a less visible way. I believe these deviations are quite unique and can make the difference in the classifier. We should indeed check this further if we want to understand to origins of the good results.

% {\bf Side note:} one way to investigate further would be to see indeed to what extent the ``compression", i.e. the small number of log-bins, affects the quality of the classifier. Another quite promising further research direction, would be to determine the minimum number of n power spectra, which should be taken into account to reach a target level of correct classification. This would be useful to determine what should be the most adequate sample size for a certain level of identification. This level might of course vary as a function of subjects and tasks.

%-----------


% \textcolor{red}{\bf [Maybe a schema would be great to help the reader get the point quickly]}

At this point, we have a one-dimensional signal flattened in the time dimension, computing the mean magnitude of each bin over all readings in the recording - a row vector with one entry for each each measured frequncy bin. 

\subsection{Classifier}

Support vector machines (SVM) are a set of supervised machine learning methods that take labeled example data to create a model that can be used to predict the classes of unlabeled data. SVMs use a hyperplane (an n-dimensional construct in n+1 dimensional space) to draw discriminatory boundaries between classes. In contrast to linear discriminant analysis, which has a long history of use in BCIs, SVMs select the hyperplane that maximizes distance from the nearest training points, which has been shown to increase the model's generalizability \cite{burges_tutorial_1998}.  For more on SVM's in BCI: \cite{garrett_comparison_2003,grierson_better_2011} 

\textcolor{red}{a diagram could help understand how SVM works}

In this study, we use LinearSVC, \cite{fan_liblinear:_2008} a wrapper for LibLinear exposed in Python through the ScikitLearn library. \cite{pedregosa_scikit-learn:_2011} We chose LinearSVC primarily because its underlying C implementation is very performant, and because linear kernels performed as well or better than nonlinear ones in early experimentation, corroborating the findings of previous studies. \cite{garrett_comparison_2003,lotte_review_2007}  We use the default settings for LinearSVC: a C of 1.0, squared hinge loss function, and a tolerance parameter of of 1e-4. 

% TODO:  explain what these parameters are, by e.g., providing the main formula of the LinearSVC.
% TODO: provide evidence for "as well or better than nonlinear ones"

\subsection{Experiment 1}

For each subject, we generated every pair of two tasks (a binary BCI) and cross-validated our SVM seven times, the recommended default from scikit-learn, on recordings for those two tasks. We used ScikitLearn's built-in cross-validation toolkit, which was configured to perform each of the seven cross-validation steps using different splits of trial data in the training and testing sets. We varied the number of bins in the samples we fed to the SVM and the length of recordings (a slice of our original recording from one second to 1+n seconds). For every task pair processed, we recorded mean classification accuracy across all cross-validation trials.

As an additional performance audit, we timed our SVM at training time and testing time. In this measure, we fit an SVM to all the data for two task-pairs from five randomly-selected subjects, and repeated this process ten thousand times at different bin sizes and different lengths, collecting the minimum time observed in each series of attempts. In order to establish a proper estimate, we time the SVM after all data has been loaded to memory, disregarding the time it takes to load the data from disk.

\subsection{Experiment 2}

For each subject, we spliced all recordings into 0.5-second chunks, each one representing a single power spectrum reading from our headset. We repeated our previous calibration routine of testing all possible task pairs; however, in contrast to our methods in the last experiment, we trained the SVM only on the first recordings from each user (that is, data collected from the first few trials each subject performed), and tested the SVM only on recordings from later trials. In order to simulate the constraints of quick, naturalistic interaction, we tested on only the first reading (i.e., the first 1/2 second) in each trial. We varied the number of seconds of data used to train the SVM. We also varied the number of bins in each recording. Once again, we measured mean classifier accuracy on all items in our training set.

\subsection{Experiment 3}
Using the 0.5-second recordings from Experiment 2, we simulated the calibration process of all fourteen subjects. We began with sixty seconds of recording from the three tasks most commonly associated with best-case performance (base, pass, color). We then performed a seven-fold cross-validation on every permutation of two of these tasks (base versus task, pass versus task, pass versus color, etc). The taskpair with the highest mean score across all seven cross-validations was selected for a more robust calibration, in which the reamining 80 seconds of recordings for each taskpair were used to validate the SVM trained on the first 120 seconds of taskpair data. If the resulting score was below 75\%, we added the first 60 seconds of recording from the taskpair that was next most correlated with bestcase accuracy. This process was repeated iteratively until all taskpairs were added, or until one taskpair got over 75\% accuracy. We attempted this calibration process at a number of bin sizes and recorded the accuracy achieved among subjects and the number of seconds of training data required to achieve the resulting accuracy.